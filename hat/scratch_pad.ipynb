{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zbyang/miniconda3/envs/cuda11.7/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import json\n",
    "import numpy as np\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "semSS_strings = np.load('/data/add_disk0/zbyang/datasets/coco_search18/semantic_seq_full/test.pkl', allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.load('/data/add_disk0/zbyang/datasets/coco_search18/semantic_seq_full/test_TA_Sem.pkl', allow_pickle=True)\n",
    "b = np.load('/data/add_disk0/zbyang/datasets/coco_search18/semantic_seq_full/test_TP_Sem.pkl', allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "aa = {}\n",
    "for k, v in a.items():\n",
    "    x = []\n",
    "    for y in v:\n",
    "        x.append([yy[0] for yy in y])\n",
    "    aa[k] = x\n",
    "bb = {}\n",
    "for k, v in b.items():\n",
    "    x = []\n",
    "    for y in v:\n",
    "        x.append([yy[0] for yy in y])\n",
    "    bb[k] = x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(612, 612, 1224)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cc = aa.copy()\n",
    "cc.update(bb)\n",
    "len(aa),len(bb),len(cc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('/data/add_disk0/zbyang/datasets/coco_search18/semantic_seq_full/test.pkl', 'wb') as f:\n",
    "    pickle.dump(cc, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "61839"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(os.path.join('/home/zbyang/coco_data_release/COCOFreeView-fixations.json')) as json_file:\n",
    "    fv_sps = json.load(json_file)\n",
    "len(fv_sps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3714 623 1127\n",
      "20 32 97\n"
     ]
    }
   ],
   "source": [
    "img_names_train = np.unique([x['name'] for x in fv_sps if x['split'] == 'train'])\n",
    "img_names_valid = np.unique([x['name'] for x in fv_sps if x['split'] == 'valid'])\n",
    "img_names_test = np.unique([x['name'] for x in fv_sps if x['split'] == 'test'])\n",
    "print(len(img_names_train), len(img_names_valid), len(img_names_test))\n",
    "print(len(set(img_names_train) & set(img_names_valid)), len(set(img_names_train) & set(img_names_test)), len(set(img_names_valid) & set(img_names_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 61839/61839 [00:23<00:00, 2619.58it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "60262"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove the images that are in both train and valid/test\n",
    "new_fv_sps = []\n",
    "for sp in tqdm(fv_sps):\n",
    "    sp.pop('task')\n",
    "    if sp['split'] == 'valid' and sp['name'] in (set(img_names_train) & set(img_names_valid)):\n",
    "        continue\n",
    "    if sp['split'] == 'test' and sp['name'] in (set(img_names_train) & set(img_names_test)):\n",
    "        continue\n",
    "    if sp['split'] == 'test' and sp['name'] in (set(img_names_valid) & set(img_names_test)):\n",
    "        continue\n",
    "    new_fv_sps.append(sp)\n",
    "len(new_fv_sps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 60262/60262 [00:00<00:00, 1280677.88it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(60262, 53015)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# deduplication\n",
    "sps = {}\n",
    "for sp in tqdm(new_fv_sps):\n",
    "    idf = f'{sp[\"name\"]}_{sp[\"subject\"]}'\n",
    "    sps[idf] = sp\n",
    "len(new_fv_sps), len(sps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "53015"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_fv_sps = list(sps.values())\n",
    "len(new_fv_sps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3714 603 1000\n",
      "0 0 0\n"
     ]
    }
   ],
   "source": [
    "# Double check overlapping\n",
    "img_names_train = np.unique([x['name'] for x in new_fv_sps if x['split'] == 'train'])\n",
    "img_names_valid = np.unique([x['name'] for x in new_fv_sps if x['split'] == 'valid'])\n",
    "img_names_test = np.unique([x['name'] for x in new_fv_sps if x['split'] == 'test'])\n",
    "print(len(img_names_train), len(img_names_valid), len(img_names_test))\n",
    "print(len(set(img_names_train) & set(img_names_valid)), len(set(img_names_train) & set(img_names_test)), len(set(img_names_valid) & set(img_names_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43048 9967\n"
     ]
    }
   ],
   "source": [
    "trainval_split = list(filter(lambda x: x['split'] != 'test', new_fv_sps))\n",
    "test_split = list(filter(lambda x: x['split'] == 'test', new_fv_sps))\n",
    "print(len(trainval_split), len(test_split))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/home/zbyang/coco_data_release/COCOFreeView_fixations_trainval_new.json', 'w') as json_file:\n",
    "    json.dump(trainval_split, json_file, indent=4)\n",
    "with open('/home/zbyang/coco_data_release/COCOFreeView_fixations_test_new.json', 'w') as json_file:\n",
    "    json.dump(test_split, json_file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21627 3259 6120\n"
     ]
    }
   ],
   "source": [
    "# Create a new split for COCO-Search18 according to the new train/valid/test split in COCOFreeView\n",
    "with open(os.path.join('/home/zbyang/coco_data_release/coco_search18_fixations_TA_test.json')) as json_file:\n",
    "    ta_test = json.load(json_file)\n",
    "with open(os.path.join('/home/zbyang/coco_data_release/coco_search18_fixations_TA_trainval.json')) as json_file:\n",
    "    ta_trainval = json.load(json_file)\n",
    "    ta_train = list(filter(lambda x: x['split'] == 'train', ta_trainval))\n",
    "    ta_valid = list(filter(lambda x: x['split'] == 'valid', ta_trainval))\n",
    "print(len(ta_train), len(ta_valid), len(ta_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21947 3789 5270\n"
     ]
    }
   ],
   "source": [
    "new_ta_train, new_ta_valid, new_ta_test = [], [], []\n",
    "for sp in ta_train + ta_valid + ta_test:\n",
    "    if sp['name'] in img_names_train:\n",
    "        new_ta_train.append(sp)\n",
    "    elif sp['name'] in img_names_valid:\n",
    "        new_ta_valid.append(sp)\n",
    "    elif sp['name'] in img_names_test:\n",
    "        new_ta_test.append(sp)\n",
    "print(len(new_ta_train), len(new_ta_valid), len(new_ta_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18\n",
      "18\n",
      "18\n"
     ]
    }
   ],
   "source": [
    "# Check for tasks\n",
    "print(len(np.unique([x['task'] for x in new_ta_train])))\n",
    "print(len(np.unique([x['task'] for x in new_ta_valid])))\n",
    "print(len(np.unique([x['task'] for x in new_ta_test])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.load('/data/add_disk0/zbyang/projects/scanpath_prediction_all/assets/pretrain_HAT/ckp_4999.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "odict_keys(['encoder.backbone.stem.conv1.weight', 'encoder.backbone.stem.conv1.norm.weight', 'encoder.backbone.stem.conv1.norm.bias', 'encoder.backbone.stem.conv1.norm.running_mean', 'encoder.backbone.stem.conv1.norm.running_var', 'encoder.backbone.stages.res2.0.shortcut.weight', 'encoder.backbone.stages.res2.0.shortcut.norm.weight', 'encoder.backbone.stages.res2.0.shortcut.norm.bias', 'encoder.backbone.stages.res2.0.shortcut.norm.running_mean', 'encoder.backbone.stages.res2.0.shortcut.norm.running_var', 'encoder.backbone.stages.res2.0.conv1.weight', 'encoder.backbone.stages.res2.0.conv1.norm.weight', 'encoder.backbone.stages.res2.0.conv1.norm.bias', 'encoder.backbone.stages.res2.0.conv1.norm.running_mean', 'encoder.backbone.stages.res2.0.conv1.norm.running_var', 'encoder.backbone.stages.res2.0.conv2.weight', 'encoder.backbone.stages.res2.0.conv2.norm.weight', 'encoder.backbone.stages.res2.0.conv2.norm.bias', 'encoder.backbone.stages.res2.0.conv2.norm.running_mean', 'encoder.backbone.stages.res2.0.conv2.norm.running_var', 'encoder.backbone.stages.res2.0.conv3.weight', 'encoder.backbone.stages.res2.0.conv3.norm.weight', 'encoder.backbone.stages.res2.0.conv3.norm.bias', 'encoder.backbone.stages.res2.0.conv3.norm.running_mean', 'encoder.backbone.stages.res2.0.conv3.norm.running_var', 'encoder.backbone.stages.res2.1.conv1.weight', 'encoder.backbone.stages.res2.1.conv1.norm.weight', 'encoder.backbone.stages.res2.1.conv1.norm.bias', 'encoder.backbone.stages.res2.1.conv1.norm.running_mean', 'encoder.backbone.stages.res2.1.conv1.norm.running_var', 'encoder.backbone.stages.res2.1.conv2.weight', 'encoder.backbone.stages.res2.1.conv2.norm.weight', 'encoder.backbone.stages.res2.1.conv2.norm.bias', 'encoder.backbone.stages.res2.1.conv2.norm.running_mean', 'encoder.backbone.stages.res2.1.conv2.norm.running_var', 'encoder.backbone.stages.res2.1.conv3.weight', 'encoder.backbone.stages.res2.1.conv3.norm.weight', 'encoder.backbone.stages.res2.1.conv3.norm.bias', 'encoder.backbone.stages.res2.1.conv3.norm.running_mean', 'encoder.backbone.stages.res2.1.conv3.norm.running_var', 'encoder.backbone.stages.res2.2.conv1.weight', 'encoder.backbone.stages.res2.2.conv1.norm.weight', 'encoder.backbone.stages.res2.2.conv1.norm.bias', 'encoder.backbone.stages.res2.2.conv1.norm.running_mean', 'encoder.backbone.stages.res2.2.conv1.norm.running_var', 'encoder.backbone.stages.res2.2.conv2.weight', 'encoder.backbone.stages.res2.2.conv2.norm.weight', 'encoder.backbone.stages.res2.2.conv2.norm.bias', 'encoder.backbone.stages.res2.2.conv2.norm.running_mean', 'encoder.backbone.stages.res2.2.conv2.norm.running_var', 'encoder.backbone.stages.res2.2.conv3.weight', 'encoder.backbone.stages.res2.2.conv3.norm.weight', 'encoder.backbone.stages.res2.2.conv3.norm.bias', 'encoder.backbone.stages.res2.2.conv3.norm.running_mean', 'encoder.backbone.stages.res2.2.conv3.norm.running_var', 'encoder.backbone.stages.res3.0.shortcut.weight', 'encoder.backbone.stages.res3.0.shortcut.norm.weight', 'encoder.backbone.stages.res3.0.shortcut.norm.bias', 'encoder.backbone.stages.res3.0.shortcut.norm.running_mean', 'encoder.backbone.stages.res3.0.shortcut.norm.running_var', 'encoder.backbone.stages.res3.0.conv1.weight', 'encoder.backbone.stages.res3.0.conv1.norm.weight', 'encoder.backbone.stages.res3.0.conv1.norm.bias', 'encoder.backbone.stages.res3.0.conv1.norm.running_mean', 'encoder.backbone.stages.res3.0.conv1.norm.running_var', 'encoder.backbone.stages.res3.0.conv2.weight', 'encoder.backbone.stages.res3.0.conv2.norm.weight', 'encoder.backbone.stages.res3.0.conv2.norm.bias', 'encoder.backbone.stages.res3.0.conv2.norm.running_mean', 'encoder.backbone.stages.res3.0.conv2.norm.running_var', 'encoder.backbone.stages.res3.0.conv3.weight', 'encoder.backbone.stages.res3.0.conv3.norm.weight', 'encoder.backbone.stages.res3.0.conv3.norm.bias', 'encoder.backbone.stages.res3.0.conv3.norm.running_mean', 'encoder.backbone.stages.res3.0.conv3.norm.running_var', 'encoder.backbone.stages.res3.1.conv1.weight', 'encoder.backbone.stages.res3.1.conv1.norm.weight', 'encoder.backbone.stages.res3.1.conv1.norm.bias', 'encoder.backbone.stages.res3.1.conv1.norm.running_mean', 'encoder.backbone.stages.res3.1.conv1.norm.running_var', 'encoder.backbone.stages.res3.1.conv2.weight', 'encoder.backbone.stages.res3.1.conv2.norm.weight', 'encoder.backbone.stages.res3.1.conv2.norm.bias', 'encoder.backbone.stages.res3.1.conv2.norm.running_mean', 'encoder.backbone.stages.res3.1.conv2.norm.running_var', 'encoder.backbone.stages.res3.1.conv3.weight', 'encoder.backbone.stages.res3.1.conv3.norm.weight', 'encoder.backbone.stages.res3.1.conv3.norm.bias', 'encoder.backbone.stages.res3.1.conv3.norm.running_mean', 'encoder.backbone.stages.res3.1.conv3.norm.running_var', 'encoder.backbone.stages.res3.2.conv1.weight', 'encoder.backbone.stages.res3.2.conv1.norm.weight', 'encoder.backbone.stages.res3.2.conv1.norm.bias', 'encoder.backbone.stages.res3.2.conv1.norm.running_mean', 'encoder.backbone.stages.res3.2.conv1.norm.running_var', 'encoder.backbone.stages.res3.2.conv2.weight', 'encoder.backbone.stages.res3.2.conv2.norm.weight', 'encoder.backbone.stages.res3.2.conv2.norm.bias', 'encoder.backbone.stages.res3.2.conv2.norm.running_mean', 'encoder.backbone.stages.res3.2.conv2.norm.running_var', 'encoder.backbone.stages.res3.2.conv3.weight', 'encoder.backbone.stages.res3.2.conv3.norm.weight', 'encoder.backbone.stages.res3.2.conv3.norm.bias', 'encoder.backbone.stages.res3.2.conv3.norm.running_mean', 'encoder.backbone.stages.res3.2.conv3.norm.running_var', 'encoder.backbone.stages.res3.3.conv1.weight', 'encoder.backbone.stages.res3.3.conv1.norm.weight', 'encoder.backbone.stages.res3.3.conv1.norm.bias', 'encoder.backbone.stages.res3.3.conv1.norm.running_mean', 'encoder.backbone.stages.res3.3.conv1.norm.running_var', 'encoder.backbone.stages.res3.3.conv2.weight', 'encoder.backbone.stages.res3.3.conv2.norm.weight', 'encoder.backbone.stages.res3.3.conv2.norm.bias', 'encoder.backbone.stages.res3.3.conv2.norm.running_mean', 'encoder.backbone.stages.res3.3.conv2.norm.running_var', 'encoder.backbone.stages.res3.3.conv3.weight', 'encoder.backbone.stages.res3.3.conv3.norm.weight', 'encoder.backbone.stages.res3.3.conv3.norm.bias', 'encoder.backbone.stages.res3.3.conv3.norm.running_mean', 'encoder.backbone.stages.res3.3.conv3.norm.running_var', 'encoder.backbone.stages.res4.0.shortcut.weight', 'encoder.backbone.stages.res4.0.shortcut.norm.weight', 'encoder.backbone.stages.res4.0.shortcut.norm.bias', 'encoder.backbone.stages.res4.0.shortcut.norm.running_mean', 'encoder.backbone.stages.res4.0.shortcut.norm.running_var', 'encoder.backbone.stages.res4.0.conv1.weight', 'encoder.backbone.stages.res4.0.conv1.norm.weight', 'encoder.backbone.stages.res4.0.conv1.norm.bias', 'encoder.backbone.stages.res4.0.conv1.norm.running_mean', 'encoder.backbone.stages.res4.0.conv1.norm.running_var', 'encoder.backbone.stages.res4.0.conv2.weight', 'encoder.backbone.stages.res4.0.conv2.norm.weight', 'encoder.backbone.stages.res4.0.conv2.norm.bias', 'encoder.backbone.stages.res4.0.conv2.norm.running_mean', 'encoder.backbone.stages.res4.0.conv2.norm.running_var', 'encoder.backbone.stages.res4.0.conv3.weight', 'encoder.backbone.stages.res4.0.conv3.norm.weight', 'encoder.backbone.stages.res4.0.conv3.norm.bias', 'encoder.backbone.stages.res4.0.conv3.norm.running_mean', 'encoder.backbone.stages.res4.0.conv3.norm.running_var', 'encoder.backbone.stages.res4.1.conv1.weight', 'encoder.backbone.stages.res4.1.conv1.norm.weight', 'encoder.backbone.stages.res4.1.conv1.norm.bias', 'encoder.backbone.stages.res4.1.conv1.norm.running_mean', 'encoder.backbone.stages.res4.1.conv1.norm.running_var', 'encoder.backbone.stages.res4.1.conv2.weight', 'encoder.backbone.stages.res4.1.conv2.norm.weight', 'encoder.backbone.stages.res4.1.conv2.norm.bias', 'encoder.backbone.stages.res4.1.conv2.norm.running_mean', 'encoder.backbone.stages.res4.1.conv2.norm.running_var', 'encoder.backbone.stages.res4.1.conv3.weight', 'encoder.backbone.stages.res4.1.conv3.norm.weight', 'encoder.backbone.stages.res4.1.conv3.norm.bias', 'encoder.backbone.stages.res4.1.conv3.norm.running_mean', 'encoder.backbone.stages.res4.1.conv3.norm.running_var', 'encoder.backbone.stages.res4.2.conv1.weight', 'encoder.backbone.stages.res4.2.conv1.norm.weight', 'encoder.backbone.stages.res4.2.conv1.norm.bias', 'encoder.backbone.stages.res4.2.conv1.norm.running_mean', 'encoder.backbone.stages.res4.2.conv1.norm.running_var', 'encoder.backbone.stages.res4.2.conv2.weight', 'encoder.backbone.stages.res4.2.conv2.norm.weight', 'encoder.backbone.stages.res4.2.conv2.norm.bias', 'encoder.backbone.stages.res4.2.conv2.norm.running_mean', 'encoder.backbone.stages.res4.2.conv2.norm.running_var', 'encoder.backbone.stages.res4.2.conv3.weight', 'encoder.backbone.stages.res4.2.conv3.norm.weight', 'encoder.backbone.stages.res4.2.conv3.norm.bias', 'encoder.backbone.stages.res4.2.conv3.norm.running_mean', 'encoder.backbone.stages.res4.2.conv3.norm.running_var', 'encoder.backbone.stages.res4.3.conv1.weight', 'encoder.backbone.stages.res4.3.conv1.norm.weight', 'encoder.backbone.stages.res4.3.conv1.norm.bias', 'encoder.backbone.stages.res4.3.conv1.norm.running_mean', 'encoder.backbone.stages.res4.3.conv1.norm.running_var', 'encoder.backbone.stages.res4.3.conv2.weight', 'encoder.backbone.stages.res4.3.conv2.norm.weight', 'encoder.backbone.stages.res4.3.conv2.norm.bias', 'encoder.backbone.stages.res4.3.conv2.norm.running_mean', 'encoder.backbone.stages.res4.3.conv2.norm.running_var', 'encoder.backbone.stages.res4.3.conv3.weight', 'encoder.backbone.stages.res4.3.conv3.norm.weight', 'encoder.backbone.stages.res4.3.conv3.norm.bias', 'encoder.backbone.stages.res4.3.conv3.norm.running_mean', 'encoder.backbone.stages.res4.3.conv3.norm.running_var', 'encoder.backbone.stages.res4.4.conv1.weight', 'encoder.backbone.stages.res4.4.conv1.norm.weight', 'encoder.backbone.stages.res4.4.conv1.norm.bias', 'encoder.backbone.stages.res4.4.conv1.norm.running_mean', 'encoder.backbone.stages.res4.4.conv1.norm.running_var', 'encoder.backbone.stages.res4.4.conv2.weight', 'encoder.backbone.stages.res4.4.conv2.norm.weight', 'encoder.backbone.stages.res4.4.conv2.norm.bias', 'encoder.backbone.stages.res4.4.conv2.norm.running_mean', 'encoder.backbone.stages.res4.4.conv2.norm.running_var', 'encoder.backbone.stages.res4.4.conv3.weight', 'encoder.backbone.stages.res4.4.conv3.norm.weight', 'encoder.backbone.stages.res4.4.conv3.norm.bias', 'encoder.backbone.stages.res4.4.conv3.norm.running_mean', 'encoder.backbone.stages.res4.4.conv3.norm.running_var', 'encoder.backbone.stages.res4.5.conv1.weight', 'encoder.backbone.stages.res4.5.conv1.norm.weight', 'encoder.backbone.stages.res4.5.conv1.norm.bias', 'encoder.backbone.stages.res4.5.conv1.norm.running_mean', 'encoder.backbone.stages.res4.5.conv1.norm.running_var', 'encoder.backbone.stages.res4.5.conv2.weight', 'encoder.backbone.stages.res4.5.conv2.norm.weight', 'encoder.backbone.stages.res4.5.conv2.norm.bias', 'encoder.backbone.stages.res4.5.conv2.norm.running_mean', 'encoder.backbone.stages.res4.5.conv2.norm.running_var', 'encoder.backbone.stages.res4.5.conv3.weight', 'encoder.backbone.stages.res4.5.conv3.norm.weight', 'encoder.backbone.stages.res4.5.conv3.norm.bias', 'encoder.backbone.stages.res4.5.conv3.norm.running_mean', 'encoder.backbone.stages.res4.5.conv3.norm.running_var', 'encoder.backbone.stages.res5.0.shortcut.weight', 'encoder.backbone.stages.res5.0.shortcut.norm.weight', 'encoder.backbone.stages.res5.0.shortcut.norm.bias', 'encoder.backbone.stages.res5.0.shortcut.norm.running_mean', 'encoder.backbone.stages.res5.0.shortcut.norm.running_var', 'encoder.backbone.stages.res5.0.conv1.weight', 'encoder.backbone.stages.res5.0.conv1.norm.weight', 'encoder.backbone.stages.res5.0.conv1.norm.bias', 'encoder.backbone.stages.res5.0.conv1.norm.running_mean', 'encoder.backbone.stages.res5.0.conv1.norm.running_var', 'encoder.backbone.stages.res5.0.conv2.weight', 'encoder.backbone.stages.res5.0.conv2.norm.weight', 'encoder.backbone.stages.res5.0.conv2.norm.bias', 'encoder.backbone.stages.res5.0.conv2.norm.running_mean', 'encoder.backbone.stages.res5.0.conv2.norm.running_var', 'encoder.backbone.stages.res5.0.conv3.weight', 'encoder.backbone.stages.res5.0.conv3.norm.weight', 'encoder.backbone.stages.res5.0.conv3.norm.bias', 'encoder.backbone.stages.res5.0.conv3.norm.running_mean', 'encoder.backbone.stages.res5.0.conv3.norm.running_var', 'encoder.backbone.stages.res5.1.conv1.weight', 'encoder.backbone.stages.res5.1.conv1.norm.weight', 'encoder.backbone.stages.res5.1.conv1.norm.bias', 'encoder.backbone.stages.res5.1.conv1.norm.running_mean', 'encoder.backbone.stages.res5.1.conv1.norm.running_var', 'encoder.backbone.stages.res5.1.conv2.weight', 'encoder.backbone.stages.res5.1.conv2.norm.weight', 'encoder.backbone.stages.res5.1.conv2.norm.bias', 'encoder.backbone.stages.res5.1.conv2.norm.running_mean', 'encoder.backbone.stages.res5.1.conv2.norm.running_var', 'encoder.backbone.stages.res5.1.conv3.weight', 'encoder.backbone.stages.res5.1.conv3.norm.weight', 'encoder.backbone.stages.res5.1.conv3.norm.bias', 'encoder.backbone.stages.res5.1.conv3.norm.running_mean', 'encoder.backbone.stages.res5.1.conv3.norm.running_var', 'encoder.backbone.stages.res5.2.conv1.weight', 'encoder.backbone.stages.res5.2.conv1.norm.weight', 'encoder.backbone.stages.res5.2.conv1.norm.bias', 'encoder.backbone.stages.res5.2.conv1.norm.running_mean', 'encoder.backbone.stages.res5.2.conv1.norm.running_var', 'encoder.backbone.stages.res5.2.conv2.weight', 'encoder.backbone.stages.res5.2.conv2.norm.weight', 'encoder.backbone.stages.res5.2.conv2.norm.bias', 'encoder.backbone.stages.res5.2.conv2.norm.running_mean', 'encoder.backbone.stages.res5.2.conv2.norm.running_var', 'encoder.backbone.stages.res5.2.conv3.weight', 'encoder.backbone.stages.res5.2.conv3.norm.weight', 'encoder.backbone.stages.res5.2.conv3.norm.bias', 'encoder.backbone.stages.res5.2.conv3.norm.running_mean', 'encoder.backbone.stages.res5.2.conv3.norm.running_var', 'encoder.pixel_decoder.input_proj.0.0.weight', 'encoder.pixel_decoder.input_proj.0.0.bias', 'encoder.pixel_decoder.input_proj.0.1.weight', 'encoder.pixel_decoder.input_proj.0.1.bias', 'encoder.pixel_decoder.input_proj.1.0.weight', 'encoder.pixel_decoder.input_proj.1.0.bias', 'encoder.pixel_decoder.input_proj.1.1.weight', 'encoder.pixel_decoder.input_proj.1.1.bias', 'encoder.pixel_decoder.input_proj.2.0.weight', 'encoder.pixel_decoder.input_proj.2.0.bias', 'encoder.pixel_decoder.input_proj.2.1.weight', 'encoder.pixel_decoder.input_proj.2.1.bias', 'encoder.pixel_decoder.transformer.level_embed', 'encoder.pixel_decoder.transformer.encoder.layers.0.self_attn.sampling_offsets.weight', 'encoder.pixel_decoder.transformer.encoder.layers.0.self_attn.sampling_offsets.bias', 'encoder.pixel_decoder.transformer.encoder.layers.0.self_attn.attention_weights.weight', 'encoder.pixel_decoder.transformer.encoder.layers.0.self_attn.attention_weights.bias', 'encoder.pixel_decoder.transformer.encoder.layers.0.self_attn.value_proj.weight', 'encoder.pixel_decoder.transformer.encoder.layers.0.self_attn.value_proj.bias', 'encoder.pixel_decoder.transformer.encoder.layers.0.self_attn.output_proj.weight', 'encoder.pixel_decoder.transformer.encoder.layers.0.self_attn.output_proj.bias', 'encoder.pixel_decoder.transformer.encoder.layers.0.norm1.weight', 'encoder.pixel_decoder.transformer.encoder.layers.0.norm1.bias', 'encoder.pixel_decoder.transformer.encoder.layers.0.linear1.weight', 'encoder.pixel_decoder.transformer.encoder.layers.0.linear1.bias', 'encoder.pixel_decoder.transformer.encoder.layers.0.linear2.weight', 'encoder.pixel_decoder.transformer.encoder.layers.0.linear2.bias', 'encoder.pixel_decoder.transformer.encoder.layers.0.norm2.weight', 'encoder.pixel_decoder.transformer.encoder.layers.0.norm2.bias', 'encoder.pixel_decoder.transformer.encoder.layers.1.self_attn.sampling_offsets.weight', 'encoder.pixel_decoder.transformer.encoder.layers.1.self_attn.sampling_offsets.bias', 'encoder.pixel_decoder.transformer.encoder.layers.1.self_attn.attention_weights.weight', 'encoder.pixel_decoder.transformer.encoder.layers.1.self_attn.attention_weights.bias', 'encoder.pixel_decoder.transformer.encoder.layers.1.self_attn.value_proj.weight', 'encoder.pixel_decoder.transformer.encoder.layers.1.self_attn.value_proj.bias', 'encoder.pixel_decoder.transformer.encoder.layers.1.self_attn.output_proj.weight', 'encoder.pixel_decoder.transformer.encoder.layers.1.self_attn.output_proj.bias', 'encoder.pixel_decoder.transformer.encoder.layers.1.norm1.weight', 'encoder.pixel_decoder.transformer.encoder.layers.1.norm1.bias', 'encoder.pixel_decoder.transformer.encoder.layers.1.linear1.weight', 'encoder.pixel_decoder.transformer.encoder.layers.1.linear1.bias', 'encoder.pixel_decoder.transformer.encoder.layers.1.linear2.weight', 'encoder.pixel_decoder.transformer.encoder.layers.1.linear2.bias', 'encoder.pixel_decoder.transformer.encoder.layers.1.norm2.weight', 'encoder.pixel_decoder.transformer.encoder.layers.1.norm2.bias', 'encoder.pixel_decoder.transformer.encoder.layers.2.self_attn.sampling_offsets.weight', 'encoder.pixel_decoder.transformer.encoder.layers.2.self_attn.sampling_offsets.bias', 'encoder.pixel_decoder.transformer.encoder.layers.2.self_attn.attention_weights.weight', 'encoder.pixel_decoder.transformer.encoder.layers.2.self_attn.attention_weights.bias', 'encoder.pixel_decoder.transformer.encoder.layers.2.self_attn.value_proj.weight', 'encoder.pixel_decoder.transformer.encoder.layers.2.self_attn.value_proj.bias', 'encoder.pixel_decoder.transformer.encoder.layers.2.self_attn.output_proj.weight', 'encoder.pixel_decoder.transformer.encoder.layers.2.self_attn.output_proj.bias', 'encoder.pixel_decoder.transformer.encoder.layers.2.norm1.weight', 'encoder.pixel_decoder.transformer.encoder.layers.2.norm1.bias', 'encoder.pixel_decoder.transformer.encoder.layers.2.linear1.weight', 'encoder.pixel_decoder.transformer.encoder.layers.2.linear1.bias', 'encoder.pixel_decoder.transformer.encoder.layers.2.linear2.weight', 'encoder.pixel_decoder.transformer.encoder.layers.2.linear2.bias', 'encoder.pixel_decoder.transformer.encoder.layers.2.norm2.weight', 'encoder.pixel_decoder.transformer.encoder.layers.2.norm2.bias', 'encoder.pixel_decoder.transformer.encoder.layers.3.self_attn.sampling_offsets.weight', 'encoder.pixel_decoder.transformer.encoder.layers.3.self_attn.sampling_offsets.bias', 'encoder.pixel_decoder.transformer.encoder.layers.3.self_attn.attention_weights.weight', 'encoder.pixel_decoder.transformer.encoder.layers.3.self_attn.attention_weights.bias', 'encoder.pixel_decoder.transformer.encoder.layers.3.self_attn.value_proj.weight', 'encoder.pixel_decoder.transformer.encoder.layers.3.self_attn.value_proj.bias', 'encoder.pixel_decoder.transformer.encoder.layers.3.self_attn.output_proj.weight', 'encoder.pixel_decoder.transformer.encoder.layers.3.self_attn.output_proj.bias', 'encoder.pixel_decoder.transformer.encoder.layers.3.norm1.weight', 'encoder.pixel_decoder.transformer.encoder.layers.3.norm1.bias', 'encoder.pixel_decoder.transformer.encoder.layers.3.linear1.weight', 'encoder.pixel_decoder.transformer.encoder.layers.3.linear1.bias', 'encoder.pixel_decoder.transformer.encoder.layers.3.linear2.weight', 'encoder.pixel_decoder.transformer.encoder.layers.3.linear2.bias', 'encoder.pixel_decoder.transformer.encoder.layers.3.norm2.weight', 'encoder.pixel_decoder.transformer.encoder.layers.3.norm2.bias', 'encoder.pixel_decoder.transformer.encoder.layers.4.self_attn.sampling_offsets.weight', 'encoder.pixel_decoder.transformer.encoder.layers.4.self_attn.sampling_offsets.bias', 'encoder.pixel_decoder.transformer.encoder.layers.4.self_attn.attention_weights.weight', 'encoder.pixel_decoder.transformer.encoder.layers.4.self_attn.attention_weights.bias', 'encoder.pixel_decoder.transformer.encoder.layers.4.self_attn.value_proj.weight', 'encoder.pixel_decoder.transformer.encoder.layers.4.self_attn.value_proj.bias', 'encoder.pixel_decoder.transformer.encoder.layers.4.self_attn.output_proj.weight', 'encoder.pixel_decoder.transformer.encoder.layers.4.self_attn.output_proj.bias', 'encoder.pixel_decoder.transformer.encoder.layers.4.norm1.weight', 'encoder.pixel_decoder.transformer.encoder.layers.4.norm1.bias', 'encoder.pixel_decoder.transformer.encoder.layers.4.linear1.weight', 'encoder.pixel_decoder.transformer.encoder.layers.4.linear1.bias', 'encoder.pixel_decoder.transformer.encoder.layers.4.linear2.weight', 'encoder.pixel_decoder.transformer.encoder.layers.4.linear2.bias', 'encoder.pixel_decoder.transformer.encoder.layers.4.norm2.weight', 'encoder.pixel_decoder.transformer.encoder.layers.4.norm2.bias', 'encoder.pixel_decoder.transformer.encoder.layers.5.self_attn.sampling_offsets.weight', 'encoder.pixel_decoder.transformer.encoder.layers.5.self_attn.sampling_offsets.bias', 'encoder.pixel_decoder.transformer.encoder.layers.5.self_attn.attention_weights.weight', 'encoder.pixel_decoder.transformer.encoder.layers.5.self_attn.attention_weights.bias', 'encoder.pixel_decoder.transformer.encoder.layers.5.self_attn.value_proj.weight', 'encoder.pixel_decoder.transformer.encoder.layers.5.self_attn.value_proj.bias', 'encoder.pixel_decoder.transformer.encoder.layers.5.self_attn.output_proj.weight', 'encoder.pixel_decoder.transformer.encoder.layers.5.self_attn.output_proj.bias', 'encoder.pixel_decoder.transformer.encoder.layers.5.norm1.weight', 'encoder.pixel_decoder.transformer.encoder.layers.5.norm1.bias', 'encoder.pixel_decoder.transformer.encoder.layers.5.linear1.weight', 'encoder.pixel_decoder.transformer.encoder.layers.5.linear1.bias', 'encoder.pixel_decoder.transformer.encoder.layers.5.linear2.weight', 'encoder.pixel_decoder.transformer.encoder.layers.5.linear2.bias', 'encoder.pixel_decoder.transformer.encoder.layers.5.norm2.weight', 'encoder.pixel_decoder.transformer.encoder.layers.5.norm2.bias', 'encoder.pixel_decoder.mask_features.weight', 'encoder.pixel_decoder.mask_features.bias', 'encoder.pixel_decoder.lateral_convs.adapter_1.weight', 'encoder.pixel_decoder.lateral_convs.adapter_1.norm.weight', 'encoder.pixel_decoder.lateral_convs.adapter_1.norm.bias', 'encoder.pixel_decoder.output_convs.layer_1.weight', 'encoder.pixel_decoder.output_convs.layer_1.norm.weight', 'encoder.pixel_decoder.output_convs.layer_1.norm.bias', 'input_proj.weight', 'input_proj.bias', 'query_embed.weight', 'query_pos.weight', 'transformer_self_attention_layers.0.self_attn.in_proj_weight', 'transformer_self_attention_layers.0.self_attn.in_proj_bias', 'transformer_self_attention_layers.0.self_attn.out_proj.weight', 'transformer_self_attention_layers.0.self_attn.out_proj.bias', 'transformer_self_attention_layers.0.norm.weight', 'transformer_self_attention_layers.0.norm.bias', 'transformer_self_attention_layers.1.self_attn.in_proj_weight', 'transformer_self_attention_layers.1.self_attn.in_proj_bias', 'transformer_self_attention_layers.1.self_attn.out_proj.weight', 'transformer_self_attention_layers.1.self_attn.out_proj.bias', 'transformer_self_attention_layers.1.norm.weight', 'transformer_self_attention_layers.1.norm.bias', 'transformer_self_attention_layers.2.self_attn.in_proj_weight', 'transformer_self_attention_layers.2.self_attn.in_proj_bias', 'transformer_self_attention_layers.2.self_attn.out_proj.weight', 'transformer_self_attention_layers.2.self_attn.out_proj.bias', 'transformer_self_attention_layers.2.norm.weight', 'transformer_self_attention_layers.2.norm.bias', 'transformer_self_attention_layers.3.self_attn.in_proj_weight', 'transformer_self_attention_layers.3.self_attn.in_proj_bias', 'transformer_self_attention_layers.3.self_attn.out_proj.weight', 'transformer_self_attention_layers.3.self_attn.out_proj.bias', 'transformer_self_attention_layers.3.norm.weight', 'transformer_self_attention_layers.3.norm.bias', 'transformer_self_attention_layers.4.self_attn.in_proj_weight', 'transformer_self_attention_layers.4.self_attn.in_proj_bias', 'transformer_self_attention_layers.4.self_attn.out_proj.weight', 'transformer_self_attention_layers.4.self_attn.out_proj.bias', 'transformer_self_attention_layers.4.norm.weight', 'transformer_self_attention_layers.4.norm.bias', 'transformer_self_attention_layers.5.self_attn.in_proj_weight', 'transformer_self_attention_layers.5.self_attn.in_proj_bias', 'transformer_self_attention_layers.5.self_attn.out_proj.weight', 'transformer_self_attention_layers.5.self_attn.out_proj.bias', 'transformer_self_attention_layers.5.norm.weight', 'transformer_self_attention_layers.5.norm.bias', 'transformer_cross_attention_layers_dorsal.0.multihead_attn.in_proj_weight', 'transformer_cross_attention_layers_dorsal.0.multihead_attn.in_proj_bias', 'transformer_cross_attention_layers_dorsal.0.multihead_attn.out_proj.weight', 'transformer_cross_attention_layers_dorsal.0.multihead_attn.out_proj.bias', 'transformer_cross_attention_layers_dorsal.0.norm.weight', 'transformer_cross_attention_layers_dorsal.0.norm.bias', 'transformer_cross_attention_layers_dorsal.1.multihead_attn.in_proj_weight', 'transformer_cross_attention_layers_dorsal.1.multihead_attn.in_proj_bias', 'transformer_cross_attention_layers_dorsal.1.multihead_attn.out_proj.weight', 'transformer_cross_attention_layers_dorsal.1.multihead_attn.out_proj.bias', 'transformer_cross_attention_layers_dorsal.1.norm.weight', 'transformer_cross_attention_layers_dorsal.1.norm.bias', 'transformer_cross_attention_layers_dorsal.2.multihead_attn.in_proj_weight', 'transformer_cross_attention_layers_dorsal.2.multihead_attn.in_proj_bias', 'transformer_cross_attention_layers_dorsal.2.multihead_attn.out_proj.weight', 'transformer_cross_attention_layers_dorsal.2.multihead_attn.out_proj.bias', 'transformer_cross_attention_layers_dorsal.2.norm.weight', 'transformer_cross_attention_layers_dorsal.2.norm.bias', 'transformer_cross_attention_layers_dorsal.3.multihead_attn.in_proj_weight', 'transformer_cross_attention_layers_dorsal.3.multihead_attn.in_proj_bias', 'transformer_cross_attention_layers_dorsal.3.multihead_attn.out_proj.weight', 'transformer_cross_attention_layers_dorsal.3.multihead_attn.out_proj.bias', 'transformer_cross_attention_layers_dorsal.3.norm.weight', 'transformer_cross_attention_layers_dorsal.3.norm.bias', 'transformer_cross_attention_layers_dorsal.4.multihead_attn.in_proj_weight', 'transformer_cross_attention_layers_dorsal.4.multihead_attn.in_proj_bias', 'transformer_cross_attention_layers_dorsal.4.multihead_attn.out_proj.weight', 'transformer_cross_attention_layers_dorsal.4.multihead_attn.out_proj.bias', 'transformer_cross_attention_layers_dorsal.4.norm.weight', 'transformer_cross_attention_layers_dorsal.4.norm.bias', 'transformer_cross_attention_layers_dorsal.5.multihead_attn.in_proj_weight', 'transformer_cross_attention_layers_dorsal.5.multihead_attn.in_proj_bias', 'transformer_cross_attention_layers_dorsal.5.multihead_attn.out_proj.weight', 'transformer_cross_attention_layers_dorsal.5.multihead_attn.out_proj.bias', 'transformer_cross_attention_layers_dorsal.5.norm.weight', 'transformer_cross_attention_layers_dorsal.5.norm.bias', 'transformer_ffn_layers.0.linear1.weight', 'transformer_ffn_layers.0.linear1.bias', 'transformer_ffn_layers.0.linear2.weight', 'transformer_ffn_layers.0.linear2.bias', 'transformer_ffn_layers.0.norm.weight', 'transformer_ffn_layers.0.norm.bias', 'transformer_ffn_layers.1.linear1.weight', 'transformer_ffn_layers.1.linear1.bias', 'transformer_ffn_layers.1.linear2.weight', 'transformer_ffn_layers.1.linear2.bias', 'transformer_ffn_layers.1.norm.weight', 'transformer_ffn_layers.1.norm.bias', 'transformer_ffn_layers.2.linear1.weight', 'transformer_ffn_layers.2.linear1.bias', 'transformer_ffn_layers.2.linear2.weight', 'transformer_ffn_layers.2.linear2.bias', 'transformer_ffn_layers.2.norm.weight', 'transformer_ffn_layers.2.norm.bias', 'transformer_ffn_layers.3.linear1.weight', 'transformer_ffn_layers.3.linear1.bias', 'transformer_ffn_layers.3.linear2.weight', 'transformer_ffn_layers.3.linear2.bias', 'transformer_ffn_layers.3.norm.weight', 'transformer_ffn_layers.3.norm.bias', 'transformer_ffn_layers.4.linear1.weight', 'transformer_ffn_layers.4.linear1.bias', 'transformer_ffn_layers.4.linear2.weight', 'transformer_ffn_layers.4.linear2.bias', 'transformer_ffn_layers.4.norm.weight', 'transformer_ffn_layers.4.norm.bias', 'transformer_ffn_layers.5.linear1.weight', 'transformer_ffn_layers.5.linear1.bias', 'transformer_ffn_layers.5.linear2.weight', 'transformer_ffn_layers.5.linear2.bias', 'transformer_ffn_layers.5.norm.weight', 'transformer_ffn_layers.5.norm.bias', 'working_memory_encoder.layers.0.self_attn.in_proj_weight', 'working_memory_encoder.layers.0.self_attn.in_proj_bias', 'working_memory_encoder.layers.0.self_attn.out_proj.weight', 'working_memory_encoder.layers.0.self_attn.out_proj.bias', 'working_memory_encoder.layers.0.linear1.weight', 'working_memory_encoder.layers.0.linear1.bias', 'working_memory_encoder.layers.0.linear2.weight', 'working_memory_encoder.layers.0.linear2.bias', 'working_memory_encoder.layers.0.norm1.weight', 'working_memory_encoder.layers.0.norm1.bias', 'working_memory_encoder.layers.0.norm2.weight', 'working_memory_encoder.layers.0.norm2.bias', 'working_memory_encoder.layers.1.self_attn.in_proj_weight', 'working_memory_encoder.layers.1.self_attn.in_proj_bias', 'working_memory_encoder.layers.1.self_attn.out_proj.weight', 'working_memory_encoder.layers.1.self_attn.out_proj.bias', 'working_memory_encoder.layers.1.linear1.weight', 'working_memory_encoder.layers.1.linear1.bias', 'working_memory_encoder.layers.1.linear2.weight', 'working_memory_encoder.layers.1.linear2.bias', 'working_memory_encoder.layers.1.norm1.weight', 'working_memory_encoder.layers.1.norm1.bias', 'working_memory_encoder.layers.1.norm2.weight', 'working_memory_encoder.layers.1.norm2.bias', 'working_memory_encoder.layers.2.self_attn.in_proj_weight', 'working_memory_encoder.layers.2.self_attn.in_proj_bias', 'working_memory_encoder.layers.2.self_attn.out_proj.weight', 'working_memory_encoder.layers.2.self_attn.out_proj.bias', 'working_memory_encoder.layers.2.linear1.weight', 'working_memory_encoder.layers.2.linear1.bias', 'working_memory_encoder.layers.2.linear2.weight', 'working_memory_encoder.layers.2.linear2.bias', 'working_memory_encoder.layers.2.norm1.weight', 'working_memory_encoder.layers.2.norm1.bias', 'working_memory_encoder.layers.2.norm2.weight', 'working_memory_encoder.layers.2.norm2.bias', 'decoder_norm.weight', 'decoder_norm.bias', 'termination_predictor.layers.0.weight', 'termination_predictor.layers.0.bias', 'termination_predictor.layers.1.weight', 'termination_predictor.layers.1.bias', 'termination_predictor.layers.2.weight', 'termination_predictor.layers.2.bias', 'fixation_embed.layers.0.weight', 'fixation_embed.layers.0.bias', 'fixation_embed.layers.1.weight', 'fixation_embed.layers.1.bias', 'fixation_embed.layers.2.weight', 'fixation_embed.layers.2.bias', 'pixel_loc_emb.pos_w_embedding', 'pixel_loc_emb.pos_h_embedding', 'fix_ind_emb.weight', 'dorsal_ind_emb.weight', 'ventral_ind_emb.weight', 'pretrain_task_head.layers.0.weight', 'pretrain_task_head.layers.0.bias', 'pretrain_task_head.layers.1.weight', 'pretrain_task_head.layers.1.bias', 'pretrain_task_head.layers.2.weight', 'pretrain_task_head.layers.2.bias'])\n"
     ]
    }
   ],
   "source": [
    "a['model'].keys()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/data/add_disk0/zbyang/datasets/refcoco/refcocogaze_train_correct_512X320.json') as f:\n",
    "    data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(507), tensor(7), tensor(320), tensor(0))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xs.max(), Xs.min(), Ys.max(), Ys.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xs, Ys = [], []\n",
    "for d in data:\n",
    "    Xs.append(torch.tensor(d['FIX_X']))\n",
    "    Ys.append(torch.tensor(d['FIX_Y']))\n",
    "Xs = torch.cat(Xs)\n",
    "Ys = torch.cat(Ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function torch.functional.boolean_dispatch.<locals>.fn(*args, **kwargs)>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cuda11.7",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
